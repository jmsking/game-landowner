1. 出牌满足规定的规则
2. 有癞子（天地癞子）
3. 出牌合乎基本逻辑
4. 考虑合作者和竞争者

阶段一：
1. 基于Single-Agent Deep-Q Network进行出牌Action的学习
   1.1. 每次基于完整的Episodic进行样本的积累
   1.2. 构建一个四层网络（二层隐藏层网络），即Deep-Q Network，每次基于积累的样本进行出牌动作的学习
   1.3. 为了网络的稳定性，构建第二个同样结构的网络，生成第一层网路Target
   1.4. 为了网络更快的收敛，采用Experience Replay
2. 完成外部接口封装

阶段二：
1. 扩展为Multi-Agent RL模型
   1.1. 目前考虑基于Friend-Foe RL算法进行扩展
   1.2. 考虑合作者和竞争者，丰富环境因素
   1.3. 增加癞子牌型

阶段三：
1. 基于线上样本进行增量学习，提高Agent能力
2. 代码说明文档编写